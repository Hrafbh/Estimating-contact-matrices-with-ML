{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibliothéques\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_all=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\dataset_contact_norm_all.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "train_data_all=train_data_all.to_numpy() #transfomer la data from dataframe to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_home=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\dataset_contact_norm_home.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "train_data_home=train_data_home.to_numpy() #transfomer la data from dataframe to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_others=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\dataset_contact_norm_others.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "train_data_others=train_data_others.to_numpy() #transfomer la data from dataframe to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_school=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\dataset_contact_norm_school.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "train_data_school=train_data_school.to_numpy() #transfomer la data from dataframe to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_work=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\dataset_contact_norm_work.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "train_data_work=train_data_work.to_numpy() #transfomer la data from dataframe to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.39921974  1.50614776  1.43345477 ...  1.34517137  1.33938071\n",
      "   1.36080004]\n",
      " [-0.54794086 -0.54342833 -0.53012429 ... -0.51553019 -0.50175885\n",
      "  -0.50135333]\n",
      " [ 1.20513599  1.20541442  1.21588479 ...  1.22886046  1.23557319\n",
      "   1.23492397]\n",
      " ...\n",
      " [-0.43416628 -0.43761028 -0.43889767 ... -0.44614927 -0.439399\n",
      "  -0.43457953]\n",
      " [-0.99243989 -0.99248267 -0.99249703 ... -0.99258308 -0.99250197\n",
      "  -0.99244467]\n",
      " [-0.99870808 -0.99871216 -0.99871145 ... -0.99871054 -0.99871023\n",
      "  -0.99871032]]\n"
     ]
    }
   ],
   "source": [
    "test_set_x_orig_all=np.zeros((30,64))\n",
    "\n",
    "X_test_all=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\input_seri_norm_all.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "X_test_all=X_test_all.to_numpy()\n",
    "\n",
    "for i in range (30):\n",
    "    for j in range(1,65):\n",
    "                test_set_x_orig_all[i][j-1]=X_test_all[i][j]\n",
    "    \n",
    "test_set_x_orig_all=np.transpose(test_set_x_orig_all) \n",
    "m=np.mean(test_set_x_orig_all) #moyenne\n",
    "v=np.std(test_set_x_orig_all) #écart type\n",
    "test_set_x_orig_all=(test_set_x_orig_all-m)/v # centralize, strandarize the data\n",
    "print(test_set_x_orig_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.63356791  1.54635231  1.64627904 ...  1.57395133  1.58129337\n",
      "   1.64624106]\n",
      " [-0.5939649  -0.58432169 -0.58719825 ... -0.58242062 -0.58008904\n",
      "  -0.58670278]\n",
      " [ 0.65163497  0.67746449  0.65739343 ...  0.67428952  0.67552695\n",
      "   0.65761246]\n",
      " ...\n",
      " [-0.54896538 -0.53438599 -0.54844931 ... -0.55135003 -0.55144053\n",
      "  -0.55289992]\n",
      " [-0.86725057 -0.86707726 -0.86724445 ... -0.86727891 -0.86727999\n",
      "  -0.86729736]\n",
      " [-0.870805   -0.87079366 -0.8708067  ... -0.87080433 -0.87080537\n",
      "  -0.87080957]]\n"
     ]
    }
   ],
   "source": [
    "test_set_x_orig_home=np.zeros((30,64))\n",
    "\n",
    "X_test_home=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\input_seri_norm_home.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "X_test_home=X_test_home.to_numpy()\n",
    "\n",
    "for i in range (30):\n",
    "    for j in range(1,65):\n",
    "                test_set_x_orig_home[i][j-1]=X_test_home[i][j]\n",
    "    \n",
    "test_set_x_orig_home=np.transpose(test_set_x_orig_home) \n",
    "m=np.mean(test_set_x_orig_home) #moyenne\n",
    "v=np.std(test_set_x_orig_home) #écart type\n",
    "test_set_x_orig_home=(test_set_x_orig_home-m)/v # centralize, strandarize the data\n",
    "print(test_set_x_orig_home) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.45220544  2.40043366  2.05777633 ...  1.66852408  1.63902744\n",
      "   1.67570861]\n",
      " [-0.75133174 -0.72872747 -0.67747875 ... -0.61287053 -0.59204995\n",
      "  -0.58822057]\n",
      " [-0.59514043 -0.48974593 -0.12961364 ...  0.32396457  0.41778276\n",
      "   0.4252494 ]\n",
      " ...\n",
      " [-0.54014014 -0.52928546 -0.52185189 ... -0.52434211 -0.52401824\n",
      "  -0.51799634]\n",
      " [-0.77526413 -0.77513492 -0.77504644 ... -0.77507608 -0.77507223\n",
      "  -0.77500055]\n",
      " [-0.77785055 -0.7778755  -0.77785944 ... -0.77785189 -0.77785581\n",
      "  -0.77785818]]\n"
     ]
    }
   ],
   "source": [
    "test_set_x_orig_others=np.zeros((30,64))\n",
    "\n",
    "X_test_others=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\input_seri_norm_others.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "X_test_others=X_test_others.to_numpy()\n",
    "\n",
    "for i in range (30):\n",
    "    for j in range(1,65):\n",
    "                test_set_x_orig_others[i][j-1]=X_test_others[i][j]\n",
    "    \n",
    "test_set_x_orig_others=np.transpose(test_set_x_orig_others) \n",
    "m=np.mean(test_set_x_orig_others) #moyenne\n",
    "v=np.std(test_set_x_orig_others) #écart type\n",
    "test_set_x_orig_others=(test_set_x_orig_others-m)/v # centralize, strandarize the data\n",
    "print(test_set_x_orig_others) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.73205123  1.73205123  1.73205123 ...  1.73205123  1.73205123\n",
      "   1.73205123]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " ...\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "test_set_x_orig_school=np.zeros((30,64))\n",
    "\n",
    "X_test_school=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\input_seri_norm_school.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "X_test_school=X_test_school.to_numpy()\n",
    "\n",
    "for i in range (30):\n",
    "    for j in range(1,65):\n",
    "                test_set_x_orig_school[i][j-1]=X_test_school[i][j]\n",
    "    \n",
    "test_set_x_orig_school=np.transpose(test_set_x_orig_school) \n",
    "m=np.mean(test_set_x_orig_school) #moyenne\n",
    "v=np.std(test_set_x_orig_school) #écart type\n",
    "test_set_x_orig_school=(test_set_x_orig_school-m)/v # centralize, strandarize the data\n",
    "print(test_set_x_orig_school) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.19377355  2.34871739  2.17066748 ...  2.15943872  2.11232023\n",
      "   2.09873413]\n",
      " [-0.6998453  -0.71301605 -0.69774595 ... -0.69707764 -0.69303002\n",
      "  -0.69143001]\n",
      " [-0.42596981 -0.53058531 -0.41215699 ... -0.40880064 -0.38061644\n",
      "  -0.36903919]\n",
      " ...\n",
      " [-0.63196222 -0.65715172 -0.62994487 ... -0.62993515 -0.6215831\n",
      "  -0.61853051]\n",
      " [-0.752004   -0.75230388 -0.75197999 ... -0.75197987 -0.75188046\n",
      "  -0.75184412]\n",
      " [-0.75332991 -0.75337254 -0.75332569 ... -0.75332479 -0.75331266\n",
      "  -0.7533085 ]]\n"
     ]
    }
   ],
   "source": [
    "test_set_x_orig_work=np.zeros((30,64))\n",
    "\n",
    "X_test_work=pd.read_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\input_seri_norm_work.xlsx\")#Rq:génerer le dataset avec le programme dataset.py pour charger le dataset\n",
    "X_test_work=X_test_work.to_numpy()\n",
    "\n",
    "for i in range (30):\n",
    "    for j in range(1,65):\n",
    "                test_set_x_orig_work[i][j-1]=X_test_work[i][j]\n",
    "    \n",
    "test_set_x_orig_work=np.transpose(test_set_x_orig_work) \n",
    "m=np.mean(test_set_x_orig_work) #moyenne\n",
    "v=np.std(test_set_x_orig_work) #écart type\n",
    "test_set_x_orig_work=(test_set_x_orig_work-m)/v # centralize, strandarize the data\n",
    "print(test_set_x_orig_work) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.57539565  1.57049623  1.37455536 ...  1.36207176  1.43635418\n",
      "   1.37242273]\n",
      " [-0.55023404 -0.56108547 -0.50588492 ... -0.50591601 -0.53255848\n",
      "  -0.50416959]\n",
      " [ 1.18849301  1.18698878  1.22510977 ...  1.22532381  1.20789971\n",
      "   1.22491981]\n",
      " ...\n",
      " [-0.43461543 -0.44666653 -0.436347   ... -0.43494565 -0.44095501\n",
      "  -0.43039404]\n",
      " [-0.99081159 -0.99095781 -0.99083144 ... -0.99081456 -0.99088724\n",
      "  -0.9907602 ]\n",
      " [-0.99706131 -0.99706356 -0.99705924 ... -0.99705871 -0.99706004\n",
      "  -0.99705846]]\n"
     ]
    }
   ],
   "source": [
    "# importation de la data d'entrenement , on va entrainer notre modele avec 40 instances\n",
    "train_set_x_orig_all=np.zeros((120,64))\n",
    "for i in range(120):\n",
    "    for j in range(1,65):\n",
    "        train_set_x_orig_all[i][j-1]=train_data_all[i][j]\n",
    "train_set_x_orig_all=np.transpose(train_set_x_orig_all)          \n",
    "m=np.mean(train_set_x_orig_all)  #moyenne\n",
    "v=np.std(train_set_x_orig_all) #écart type\n",
    "train_set_x_orig_all=(train_set_x_orig_all-m)/v # centralize, strandarize the data\n",
    "#train_set_x_orig=(train_set_x_orig)/np.max(train_set_x_orig)\n",
    "print(train_set_x_orig_all) # train_set_x_orig est un array de taille (40,64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.55638851  1.84818384  1.53068931 ...  1.58835812  1.67576781\n",
      "   1.57883946]\n",
      " [-0.58829942 -0.60121183 -0.57601364 ... -0.58345604 -0.58886344\n",
      "  -0.5815524 ]\n",
      " [ 0.65048119  0.56673499  0.66580058 ...  0.64920515  0.62677063\n",
      "   0.65268191]\n",
      " ...\n",
      " [-0.53387363 -0.56072318 -0.54446339 ... -0.55090283 -0.55381155\n",
      "  -0.54783421]\n",
      " [-0.86167281 -0.86199214 -0.86179871 ... -0.86187527 -0.86190989\n",
      "  -0.86183879]\n",
      " [-0.86533476 -0.86536557 -0.86534226 ... -0.86534773 -0.86535375\n",
      "  -0.86534604]]\n"
     ]
    }
   ],
   "source": [
    "# importation de la data d'entrenement , on va entrainer notre modele avec 40 instances\n",
    "train_set_x_orig_home=np.zeros((120,64))\n",
    "for i in range(120):\n",
    "    for j in range(1,65):\n",
    "        train_set_x_orig_home[i][j-1]=train_data_home[i][j]\n",
    "train_set_x_orig_home=np.transpose(train_set_x_orig_home)          \n",
    "m=np.mean(train_set_x_orig_home)  #moyenne\n",
    "v=np.std(train_set_x_orig_home) #écart type\n",
    "train_set_x_orig_home=(train_set_x_orig_home-m)/v # centralize, strandarize the data\n",
    "#train_set_x_orig=(train_set_x_orig)/np.max(train_set_x_orig)\n",
    "print(train_set_x_orig_home) # train_set_x_orig est un array de taille (40,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.57608743  2.60228882  1.73871752 ...  1.69499657  2.24345273\n",
      "   1.74579303]\n",
      " [-0.75883174 -0.76338386 -0.61622109 ... -0.60734298 -0.70481997\n",
      "  -0.61386801]\n",
      " [-0.68056666 -0.7079296   0.28343989 ...  0.33747457 -0.33376585\n",
      "   0.29094394]\n",
      " ...\n",
      " [-0.54632986 -0.5460014  -0.52527987 ... -0.52344494 -0.53330655\n",
      "  -0.52164911]\n",
      " [-0.77540448 -0.77540057 -0.77515392 ... -0.77513208 -0.77524946\n",
      "  -0.7751107 ]\n",
      " [-0.77797511 -0.7779832  -0.77792759 ... -0.77792541 -0.77794071\n",
      "  -0.77792744]]\n"
     ]
    }
   ],
   "source": [
    "# importation de la data d'entrenement , on va entrainer notre modele avec 40 instances\n",
    "train_set_x_orig_others=np.zeros((120,64))\n",
    "for i in range(120):\n",
    "    for j in range(1,65):\n",
    "        train_set_x_orig_others[i][j-1]=train_data_others[i][j]\n",
    "train_set_x_orig_others=np.transpose(train_set_x_orig_others)          \n",
    "m=np.mean(train_set_x_orig_others)  #moyenne\n",
    "v=np.std(train_set_x_orig_others) #écart type\n",
    "train_set_x_orig_others=(train_set_x_orig_others-m)/v # centralize, strandarize the data\n",
    "#train_set_x_orig=(train_set_x_orig)/np.max(train_set_x_orig)\n",
    "print(train_set_x_orig_others) # train_set_x_orig est un array de taille (40,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.73205123  1.73205123  1.73205123 ...  1.73205123  1.73205123\n",
      "   1.73205123]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " ...\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]\n",
      " [-0.57735027 -0.57735027 -0.57735027 ... -0.57735027 -0.57735027\n",
      "  -0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "# importation de la data d'entrenement , on va entrainer notre modele avec 40 instances\n",
    "train_set_x_orig_school=np.zeros((120,64))\n",
    "for i in range(120):\n",
    "    for j in range(1,65):\n",
    "        train_set_x_orig_school[i][j-1]=train_data_school[i][j]\n",
    "train_set_x_orig_school=np.transpose(train_set_x_orig_school)          \n",
    "m=np.mean(train_set_x_orig_school)  #moyenne\n",
    "v=np.std(train_set_x_orig_school) #écart type\n",
    "train_set_x_orig_school=(train_set_x_orig_school-m)/v # centralize, strandarize the data\n",
    "#train_set_x_orig=(train_set_x_orig)/np.max(train_set_x_orig)\n",
    "print(train_set_x_orig_school) # train_set_x_orig est un array de taille (40,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39651126  2.11906309  2.21134222 ...  2.1597476   2.10019892\n",
      "   2.14273937]\n",
      " [-0.71894962 -0.69472634 -0.70340032 ... -0.69839288 -0.69292501\n",
      "  -0.6965536 ]\n",
      " [-0.55584235 -0.37087747 -0.43134768 ... -0.39538308 -0.35806734\n",
      "  -0.38244543]\n",
      " ...\n",
      " [-0.66458875 -0.61950285 -0.63675291 ... -0.62672386 -0.61524943\n",
      "  -0.62302328]\n",
      " [-0.75590013 -0.7553634  -0.75556874 ... -0.75544936 -0.75531277\n",
      "  -0.75540531]\n",
      " [-0.75693054 -0.75685586 -0.75688048 ... -0.75686629 -0.7568498\n",
      "  -0.75686144]]\n"
     ]
    }
   ],
   "source": [
    "# importation de la data d'entrenement , on va entrainer notre modele avec 40 instances\n",
    "train_set_x_orig_work=np.zeros((120,64))\n",
    "for i in range(120):\n",
    "    for j in range(1,65):\n",
    "        train_set_x_orig_work[i][j-1]=train_data_work[i][j]\n",
    "train_set_x_orig_work=np.transpose(train_set_x_orig_work)          \n",
    "m=np.mean(train_set_x_orig_work)  #moyenne\n",
    "v=np.std(train_set_x_orig_work) #écart type\n",
    "train_set_x_orig_work=(train_set_x_orig_work-m)/v # centralize, strandarize the data\n",
    "#train_set_x_orig=(train_set_x_orig)/np.max(train_set_x_orig)\n",
    "print(train_set_x_orig_work) # train_set_x_orig est un array de taille (40,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73313626 0.57702362 0.59581642 0.66425925 0.64671225 0.58637005\n",
      "  0.58890859 0.60022328 0.65088561 0.74852884 0.65612344 0.62513054\n",
      "  0.58031828 0.56902321 0.650675   0.58955976 0.57710909 0.61366271\n",
      "  0.62834797 0.62803828 0.60163527 0.59527213 0.63729526 0.63332091\n",
      "  0.64095942 0.60677708 0.61626443 0.59641625 0.62501901 0.60413887\n",
      "  0.69509899 0.65678736 0.60508866 0.59781659 0.57489031 0.62717318\n",
      "  0.61510465 0.60051255 0.57504731 0.64679208 0.59552136 0.39194788\n",
      "  0.56272328 0.61872295 0.59561212 0.57304353 0.63687338 0.62528368\n",
      "  0.61911936 0.60254561 0.60149431 0.60345175 0.67811076 0.63180817\n",
      "  0.61320527 0.58421796 0.56469159 0.59144725 0.61306818 0.64516843\n",
      "  0.62838524 0.58094926 0.56453255 0.59552799 0.60629647 0.59303849\n",
      "  0.5731058  0.58623151 0.58750584 0.58887309 0.60239815 0.58900055\n",
      "  0.63853408 0.60488282 0.6044914  0.64425851 0.66521049 1.\n",
      "  0.58946641 0.61740901 0.63491316 0.61250064 0.60319651 0.57935016\n",
      "  0.61086301 0.62834974 0.65018319 0.57393483 0.59552874 0.58762193\n",
      "  0.60098903 0.59151382 0.59461006 0.9565707  0.59908973 0.60014077\n",
      "  0.58779531 0.54994673 0.63454541 0.62635416 0.62423088 0.58857809\n",
      "  0.61568779 0.58063384 0.58483955 0.60176109 0.63050503 0.63976395\n",
      "  0.62014615 0.59470041 0.60995801 0.62592295 0.65484403 0.61823351\n",
      "  0.58579484 0.58613794 0.83231194 0.63654673 0.60711275 0.63868347]]\n"
     ]
    }
   ],
   "source": [
    "#importation des output(valeurs de contact) de la data d'entrainement\n",
    "train_set_y_m_all=np.zeros((120,1))\n",
    "for i in range(120):\n",
    "    train_set_y_m_all[i][0]=train_data_all[i][65]\n",
    "train_set_y_m_all=np.transpose(train_set_y_m_all)    \n",
    "#m=np.mean(train_set_y_wc) #moyenne\n",
    "#v=np.std(train_set_y_wc) #écart type\n",
    "#train_set_y=(train_set_y_wc-m)/v   # centralize, strandarize the data\n",
    "train_set_y_all=train_set_y_m_all/np.max(train_set_y_m_all)\n",
    "print(train_set_y_all) # train_set_y est un array de taille (40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.60896201 0.54412942 0.62051913 0.56163656 0.55630086\n",
      "  0.53318    0.55452705 0.73572183 0.70652136 0.73199132 0.73203402\n",
      "  0.5647561  0.51345564 0.61590571 0.57136099 0.52649106 0.54961414\n",
      "  0.60709625 0.52073189 0.54014619 0.52742307 0.58969818 0.59773924\n",
      "  0.67092801 0.7021122  0.5492116  0.53493417 0.55314193 0.54658256\n",
      "  0.96078411 0.71546364 0.66804416 0.58248536 0.4740285  0.70677238\n",
      "  0.57198102 0.57108658 0.53427011 0.6007298  0.54867354 0.38991609\n",
      "  0.64595856 0.53931806 0.47449502 0.59646717 0.57428122 0.60215982\n",
      "  0.72393758 0.53627418 0.53219729 0.5835453  0.68085578 0.58950802\n",
      "  0.53675144 0.69564955 0.5203977  0.54112952 0.54151151 0.79076067\n",
      "  0.71167681 0.6889692  0.6668527  0.5166626  0.59192201 0.52841215\n",
      "  0.51657903 0.58486345 0.54089352 0.54270659 0.51593758 0.59825295\n",
      "  0.57585634 0.57750082 0.64582944 0.55038639 0.58977214 0.696039\n",
      "  0.57337111 0.62078404 0.58583078 0.57892925 0.55815329 0.56624187\n",
      "  0.53452348 0.61260671 0.704917   0.6786453  0.6075075  0.55187688\n",
      "  0.54726879 0.56634263 0.51499458 0.72359942 0.5237298  0.56408404\n",
      "  0.5799355  0.54416704 0.74012751 0.6978138  0.56713942 0.57704647\n",
      "  0.75626823 0.51914776 0.61598073 0.54308057 0.59078629 0.69951532\n",
      "  0.80467859 0.53724808 0.58805178 0.57907159 0.66968983 0.78950934\n",
      "  0.65015895 0.57859494 0.48838121 0.55078174 0.56753579 0.5528942 ]]\n"
     ]
    }
   ],
   "source": [
    "#importation des output(valeurs de contact) de la data d'entrainement\n",
    "train_set_y_m_home=np.zeros((120,1))\n",
    "for i in range(120):\n",
    "    train_set_y_m_home[i][0]=train_data_home[i][65]\n",
    "train_set_y_m_home=np.transpose(train_set_y_m_home)    \n",
    "#m=np.mean(train_set_y_wc) #moyenne\n",
    "#v=np.std(train_set_y_wc) #écart type\n",
    "#train_set_y=(train_set_y_wc-m)/v   # centralize, strandarize the data\n",
    "train_set_y_home=train_set_y_m_home/np.max(train_set_y_m_home)\n",
    "print(train_set_y_home) # train_set_y est un array de taille (40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88274799 0.84943787 0.60680906 0.76830097 0.65566705 0.59188884\n",
      "  0.52168448 0.63429016 0.83565517 0.63168507 0.82659164 0.85260355\n",
      "  0.74857247 0.51002354 0.73106182 0.68063402 0.54140514 0.53567237\n",
      "  0.77410228 0.75249272 0.65558959 0.57315904 0.67664426 0.75078021\n",
      "  0.76850141 0.89910005 0.54357105 0.52070682 0.61359307 0.57793067\n",
      "  0.83919904 0.8368851  0.83386518 0.81498295 0.68476915 0.80965126\n",
      "  0.7346678  0.64170685 0.54932079 0.59862288 0.51992971 0.38398826\n",
      "  0.7450388  0.54718431 0.71258786 0.66144798 0.71361161 0.72166851\n",
      "  0.83335056 0.51458422 0.5192569  0.84882642 0.56524943 0.70162308\n",
      "  0.53831709 0.75009135 0.43542984 0.55773447 0.79417981 0.86648711\n",
      "  0.83996067 0.83083555 0.79126088 0.5100838  0.81363899 0.7358617\n",
      "  0.50403031 0.7996789  0.51595938 0.53043941 0.69564833 0.72866954\n",
      "  0.58552237 0.65339821 0.80842135 0.58435941 0.64614812 1.\n",
      "  0.67882086 0.79356656 0.62859203 0.84974423 0.70286242 0.74919185\n",
      "  0.54017661 0.64329339 0.77816613 0.8289476  0.71232181 0.64637005\n",
      "  0.63813073 0.7797448  0.49875713 0.81313418 0.50353123 0.55530636\n",
      "  0.680436   0.58593705 0.83078002 0.76969878 0.70300772 0.57287043\n",
      "  0.86223209 0.52332177 0.72438357 0.57690771 0.67219239 0.85890482\n",
      "  0.8014151  0.62515463 0.86981025 0.69843138 0.79270854 0.85794079\n",
      "  0.82789788 0.74228466 0.89578354 0.5619744  0.80876779 0.57724135]]\n"
     ]
    }
   ],
   "source": [
    "#importation des output(valeurs de contact) de la data d'entrainement\n",
    "train_set_y_m_others=np.zeros((120,1))\n",
    "for i in range(120):\n",
    "    train_set_y_m_others[i][0]=train_data_others[i][65]\n",
    "train_set_y_m_others=np.transpose(train_set_y_m_others)    \n",
    "#m=np.mean(train_set_y_wc) #moyenne\n",
    "#v=np.std(train_set_y_wc) #écart type\n",
    "#train_set_y=(train_set_y_wc-m)/v   # centralize, strandarize the data\n",
    "train_set_y_others=train_set_y_m_others/np.max(train_set_y_m_others)\n",
    "print(train_set_y_others) # train_set_y est un array de taille (40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42493642 0.25062657 0.55843092 0.56156948 0.61389504 0.53829208\n",
      "  0.57865805 0.52074    0.34934128 0.7835023  0.39907715 0.29788223\n",
      "  0.41297819 0.58365958 0.56844003 0.39967601 0.59278375 0.62817014\n",
      "  0.45603119 0.51445296 0.52860512 0.57065685 0.5466375  0.4803789\n",
      "  0.42955417 0.21170762 0.62545981 0.59600512 0.59935756 0.5835985\n",
      "  0.35859087 0.39116457 0.31173402 0.3450307  0.47786734 0.37610471\n",
      "  0.47481941 0.52210896 0.55600203 0.61365148 0.59035905 0.29886935\n",
      "  0.26121503 0.62201533 0.49730898 0.44320528 0.53169799 0.52041728\n",
      "  0.25221263 0.62843457 0.61739634 0.33901633 0.62383    0.54134899\n",
      "  0.61554844 0.29789869 0.60764643 0.58185485 0.43414982 0.31866488\n",
      "  0.33743672 0.21366752 0.19680795 0.64060578 0.38879526 0.4835084\n",
      "  0.62224355 0.34080137 0.5959554  0.58012891 0.5216592  0.424759\n",
      "  0.60740193 0.55506623 0.3717091  0.64659585 0.62089819 1.\n",
      "  0.4417027  0.42080301 0.57920211 0.34761437 0.50420917 0.35006761\n",
      "  0.6605034  0.52875601 0.40762241 0.18834798 0.43085807 0.49167524\n",
      "  0.54668372 0.39831139 0.64785867 0.98929072 0.63653024 0.60325546\n",
      "  0.4713459  0.50147307 0.30651237 0.45129221 0.52703281 0.52391272\n",
      "  0.25129302 0.59642612 0.39787823 0.5891313  0.55748391 0.35417277\n",
      "  0.30694902 0.54137769 0.35212211 0.51814006 0.47157007 0.24816889\n",
      "  0.27867912 0.39232187 0.86873732 0.64766576 0.39814323 0.63373978]]\n"
     ]
    }
   ],
   "source": [
    "#importation des output(valeurs de contact) de la data d'entrainement\n",
    "train_set_y_m_school=np.zeros((120,1))\n",
    "for i in range(120):\n",
    "    train_set_y_m_school[i][0]=train_data_school[i][65]\n",
    "train_set_y_m_school=np.transpose(train_set_y_m_school)    \n",
    "#m=np.mean(train_set_y_wc) #moyenne\n",
    "#v=np.std(train_set_y_wc) #écart type\n",
    "#train_set_y=(train_set_y_wc-m)/v   # centralize, strandarize the data\n",
    "train_set_y_school=train_set_y_m_school/np.max(train_set_y_m_school)\n",
    "print(train_set_y_school) # train_set_y est un array de taille (40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35960952 0.56698016 0.50351152 0.53902793 0.50616853 0.45657955\n",
      "  0.54373697 0.53761493 0.59642472 0.54474043 0.55960546 0.48944121\n",
      "  0.43338226 0.51712554 0.50590847 0.56278129 0.43228942 0.57653796\n",
      "  0.48136763 0.53452921 0.48633273 0.55190959 0.50523823 0.534176\n",
      "  0.53285064 0.53040464 0.53602597 0.56162678 0.49845164 0.53374186\n",
      "  0.4594961  0.56006134 0.50988831 0.53360424 0.51721757 0.39647405\n",
      "  0.4732894  0.48707415 0.46986    0.53581139 0.55046608 0.36463613\n",
      "  0.46122971 0.53620198 0.49287968 0.32435217 0.51126873 0.36673346\n",
      "  0.54714189 0.5315758  0.54814385 0.55329474 0.63370097 0.43870007\n",
      "  0.53738464 0.44554945 0.45102985 0.52117708 0.5256304  0.48970261\n",
      "  0.46174574 0.53025853 0.48388766 0.51598626 0.43984017 0.41766867\n",
      "  0.51333723 0.48362491 0.50581018 0.53327645 0.48957598 0.38970943\n",
      "  0.51077033 0.3359283  0.33548507 0.56364356 0.51484024 0.92546199\n",
      "  0.52637427 0.31602133 0.56051188 0.58061738 0.46223613 0.56853118\n",
      "  0.48821735 0.51510451 0.57536539 0.56093179 0.40440695 0.52292337\n",
      "  0.43174732 0.51473933 0.55135345 1.         0.54936974 0.54980771\n",
      "  0.35725616 0.3451129  0.59105461 0.46306954 0.45616095 0.48115436\n",
      "  0.50737245 0.51978062 0.45966909 0.48628943 0.47019589 0.5581592\n",
      "  0.37100684 0.48597398 0.54658816 0.49654079 0.48205742 0.51256274\n",
      "  0.43674803 0.48391539 0.99982724 0.53263732 0.57640242 0.54462207]]\n"
     ]
    }
   ],
   "source": [
    "#importation des output(valeurs de contact) de la data d'entrainement\n",
    "train_set_y_m_work=np.zeros((120,1))\n",
    "for i in range(120):\n",
    "    train_set_y_m_work[i][0]=train_data_work[i][65]\n",
    "train_set_y_m_work=np.transpose(train_set_y_m_work)    \n",
    "#m=np.mean(train_set_y_wc) #moyenne\n",
    "#v=np.std(train_set_y_wc) #écart type\n",
    "#train_set_y=(train_set_y_wc-m)/v   # centralize, strandarize the data\n",
    "train_set_y_work=train_set_y_m_work/np.max(train_set_y_m_work)\n",
    "print(train_set_y_work) # train_set_y est un array de taille (40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] 0.0\n"
     ]
    }
   ],
   "source": [
    "#initialisation des vecteurs w et b pour la regression , w est un vecteur de taille (64,1)\n",
    "def initialize_with_zeros(dim):\n",
    "    w=np.zeros((dim,1),dtype=float)\n",
    "    b= 0.0\n",
    "    return w, b\n",
    "w,b=initialize_with_zeros(64)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2996509635498974\n"
     ]
    }
   ],
   "source": [
    "#les étapes de propagation \"forward\" et \"backward\" pour l'apprentissage des paramètres.\n",
    "def propagate(w, b, X, Y):\n",
    "\n",
    "    m = 64\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "    #la fonction coût et son gradient pour la propagation \n",
    "    #negative log-likelihood cost for logistic regression\n",
    "    #cost=mean_squared_error(Y, A)\n",
    "    cost=-(1/m)*(np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)))\n",
    "    #gradient of the loss par rapport w\n",
    "    dw=(1/m)*np.dot(X,(A-Y).T)\n",
    "    #gradient of the loss par rapport b\n",
    "    db=(1/m)*np.sum(A-Y)\n",
    "    \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "grads, cost=propagate(w, b, train_set_x_orig_all, train_set_y_all)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': array([[-0.02318258],\n",
      "       [ 0.00427833],\n",
      "       [ 0.04841918],\n",
      "       [-0.01798832],\n",
      "       [-0.02407002],\n",
      "       [ 0.00564369],\n",
      "       [ 0.10495844],\n",
      "       [-0.00938973],\n",
      "       [-0.02211437],\n",
      "       [ 0.00422737],\n",
      "       [ 0.10045544],\n",
      "       [-0.01039306],\n",
      "       [-0.025843  ],\n",
      "       [ 0.00697495],\n",
      "       [ 0.05407764],\n",
      "       [-0.01768485],\n",
      "       [-0.02961096],\n",
      "       [ 0.00913909],\n",
      "       [ 0.03507064],\n",
      "       [-0.02249549],\n",
      "       [-0.03680029],\n",
      "       [ 0.0097602 ],\n",
      "       [ 0.02711337],\n",
      "       [-0.02561319],\n",
      "       [-0.03729876],\n",
      "       [ 0.00883621],\n",
      "       [ 0.01262496],\n",
      "       [-0.02917409],\n",
      "       [-0.03915181],\n",
      "       [ 0.0082522 ],\n",
      "       [ 0.00325137],\n",
      "       [-0.03040325],\n",
      "       [-0.03965466],\n",
      "       [ 0.00819531],\n",
      "       [ 0.00035102],\n",
      "       [-0.0306553 ],\n",
      "       [-0.03899495],\n",
      "       [ 0.00826713],\n",
      "       [-0.00337109],\n",
      "       [-0.03092921],\n",
      "       [-0.03744385],\n",
      "       [ 0.00838057],\n",
      "       [-0.02241804],\n",
      "       [-0.03187284],\n",
      "       [-0.03658504],\n",
      "       [ 0.00890081],\n",
      "       [-0.02681317],\n",
      "       [-0.03204846],\n",
      "       [-0.03770044],\n",
      "       [ 0.00838717],\n",
      "       [-0.0277349 ],\n",
      "       [-0.0320805 ],\n",
      "       [-0.03900168],\n",
      "       [ 0.00910333],\n",
      "       [-0.02926963],\n",
      "       [-0.03214281],\n",
      "       [-0.04290906],\n",
      "       [ 0.01107907],\n",
      "       [-0.03070143],\n",
      "       [-0.03220097],\n",
      "       [-0.04423653],\n",
      "       [ 0.01326062],\n",
      "       [-0.03171774],\n",
      "       [-0.03224088]]), 'b': 0.032341585021394594}\n"
     ]
    }
   ],
   "source": [
    "#Cette fonction optimise w et b en exécutant l'algorithme gradient descente\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    L=[]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w = w -learning_rate*dw\n",
    "        b = b -learning_rate*db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs \n",
    "\n",
    "# params -- dictionnaire contenant les vecteurs w et  b\n",
    "# grads -- dictionnaire contenant les gradients de la fonction de coût par rapport à w et b\n",
    "# costs -- liste de tous les coûts calculés pendant l'optimisation,\n",
    "\n",
    "params, grads, costs =optimize(w, b, train_set_x_orig_all, train_set_y_all, num_iterations=2000, learning_rate=0.009, print_cost=False)\n",
    "print(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60108897, 0.59818532, 0.62521979, 0.6358095 , 0.62183466,\n",
       "        0.62246298, 0.626542  , 0.62442025, 0.60815147, 0.6267456 ,\n",
       "        0.6080618 , 0.60036582, 0.61113014, 0.62806663, 0.63291177,\n",
       "        0.62660086, 0.62335032, 0.63165152, 0.61440893, 0.61153525,\n",
       "        0.62317632, 0.62758117, 0.62679446, 0.62293663, 0.61895332,\n",
       "        0.60011947, 0.62747035, 0.62912004, 0.62586845, 0.63061638,\n",
       "        0.61276371, 0.60985982, 0.59774525, 0.60360709, 0.61726883,\n",
       "        0.59946998, 0.61548025, 0.62456349, 0.62330419, 0.62896145,\n",
       "        0.6296777 , 0.56630083, 0.61670354, 0.62524683, 0.61112511,\n",
       "        0.60467286, 0.61864304, 0.60275481, 0.6097726 , 0.62858748,\n",
       "        0.62823479, 0.60335642, 0.62492366, 0.61499066, 0.62423603,\n",
       "        0.60853902, 0.60469728, 0.62598161, 0.60463347, 0.60484167,\n",
       "        0.5968804 , 0.60454279, 0.60751855, 0.62689975, 0.60447359,\n",
       "        0.60414921, 0.62958746, 0.60925189, 0.62627056, 0.62856922,\n",
       "        0.61723376, 0.61174787, 0.62568243, 0.61396318, 0.59246201,\n",
       "        0.62968386, 0.61962827, 0.66803679, 0.62263124, 0.59760402,\n",
       "        0.6239755 , 0.6070681 , 0.60992654, 0.61830953, 0.62851602,\n",
       "        0.6327102 , 0.62394149, 0.60815036, 0.61442164, 0.6262829 ,\n",
       "        0.61669972, 0.61054606, 0.62701254, 0.66141228, 0.6277742 ,\n",
       "        0.63511841, 0.60931482, 0.61898034, 0.61446047, 0.62731359,\n",
       "        0.61643448, 0.62687216, 0.60116393, 0.62735743, 0.61991086,\n",
       "        0.62577534, 0.61666965, 0.60426775, 0.60258249, 0.62599563,\n",
       "        0.59971345, 0.62258532, 0.60905042, 0.59618722, 0.59303421,\n",
       "        0.61319367, 0.64450319, 0.62698461, 0.61817131, 0.62624679]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fonction pour prédire la valeur de contact en utilsant les vecteurs w et b entrainées\n",
    "def predict(w, b, X):\n",
    "\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "\n",
    "    return A\n",
    "\n",
    "predict(params['w'],params['b'],train_set_x_orig_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construction du model en appllant les fonctions implémentées précedement\n",
    "def model(X_train, Y_train, X_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "\n",
    "    dim = 64\n",
    "    w,b=initialize_with_zeros(dim) \n",
    "\n",
    "    params, grads, cost = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100- np.mean(np.abs(Y_prediction_train - Y_train))*100))\n",
    "        #print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    d = {\"costs\": cost,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.299651\n",
      "Cost after iteration 100: 1.247853\n",
      "Cost after iteration 200: 1.247684\n",
      "Cost after iteration 300: 1.247649\n",
      "Cost after iteration 400: 1.247615\n",
      "Cost after iteration 500: 1.247581\n",
      "Cost after iteration 600: 1.247547\n",
      "Cost after iteration 700: 1.247513\n",
      "Cost after iteration 800: 1.247480\n",
      "Cost after iteration 900: 1.247447\n",
      "Cost after iteration 1000: 1.247414\n",
      "Cost after iteration 1100: 1.247381\n",
      "Cost after iteration 1200: 1.247349\n",
      "Cost after iteration 1300: 1.247317\n",
      "Cost after iteration 1400: 1.247285\n",
      "Cost after iteration 1500: 1.247253\n",
      "Cost after iteration 1600: 1.247222\n",
      "Cost after iteration 1700: 1.247190\n",
      "Cost after iteration 1800: 1.247159\n",
      "Cost after iteration 1900: 1.247128\n",
      "Cost after iteration 2000: 1.247098\n",
      "Cost after iteration 2100: 1.247067\n",
      "Cost after iteration 2200: 1.247037\n",
      "Cost after iteration 2300: 1.247007\n",
      "Cost after iteration 2400: 1.246977\n",
      "Cost after iteration 2500: 1.246947\n",
      "Cost after iteration 2600: 1.246918\n",
      "Cost after iteration 2700: 1.246889\n",
      "Cost after iteration 2800: 1.246860\n",
      "Cost after iteration 2900: 1.246831\n",
      "Cost after iteration 3000: 1.246802\n",
      "Cost after iteration 3100: 1.246774\n",
      "Cost after iteration 3200: 1.246745\n",
      "Cost after iteration 3300: 1.246717\n",
      "Cost after iteration 3400: 1.246689\n",
      "Cost after iteration 3500: 1.246662\n",
      "Cost after iteration 3600: 1.246634\n",
      "Cost after iteration 3700: 1.246607\n",
      "Cost after iteration 3800: 1.246580\n",
      "Cost after iteration 3900: 1.246553\n",
      "Cost after iteration 4000: 1.246526\n",
      "Cost after iteration 4100: 1.246499\n",
      "Cost after iteration 4200: 1.246473\n",
      "Cost after iteration 4300: 1.246447\n",
      "Cost after iteration 4400: 1.246421\n",
      "Cost after iteration 4500: 1.246395\n",
      "Cost after iteration 4600: 1.246369\n",
      "Cost after iteration 4700: 1.246344\n",
      "Cost after iteration 4800: 1.246318\n",
      "Cost after iteration 4900: 1.246293\n",
      "Cost after iteration 5000: 1.246268\n",
      "Cost after iteration 5100: 1.246243\n",
      "Cost after iteration 5200: 1.246219\n",
      "Cost after iteration 5300: 1.246194\n",
      "Cost after iteration 5400: 1.246170\n",
      "Cost after iteration 5500: 1.246146\n",
      "Cost after iteration 5600: 1.246122\n",
      "Cost after iteration 5700: 1.246098\n",
      "Cost after iteration 5800: 1.246074\n",
      "Cost after iteration 5900: 1.246050\n",
      "Cost after iteration 6000: 1.246027\n",
      "Cost after iteration 6100: 1.246004\n",
      "Cost after iteration 6200: 1.245981\n",
      "Cost after iteration 6300: 1.245958\n",
      "Cost after iteration 6400: 1.245935\n",
      "Cost after iteration 6500: 1.245912\n",
      "Cost after iteration 6600: 1.245890\n",
      "Cost after iteration 6700: 1.245868\n",
      "Cost after iteration 6800: 1.245846\n",
      "Cost after iteration 6900: 1.245824\n",
      "Cost after iteration 7000: 1.245802\n",
      "Cost after iteration 7100: 1.245780\n",
      "Cost after iteration 7200: 1.245758\n",
      "Cost after iteration 7300: 1.245737\n",
      "Cost after iteration 7400: 1.245716\n",
      "Cost after iteration 7500: 1.245694\n",
      "Cost after iteration 7600: 1.245673\n",
      "Cost after iteration 7700: 1.245653\n",
      "Cost after iteration 7800: 1.245632\n",
      "Cost after iteration 7900: 1.245611\n",
      "Cost after iteration 8000: 1.245591\n",
      "Cost after iteration 8100: 1.245570\n",
      "Cost after iteration 8200: 1.245550\n",
      "Cost after iteration 8300: 1.245530\n",
      "Cost after iteration 8400: 1.245510\n",
      "Cost after iteration 8500: 1.245491\n",
      "Cost after iteration 8600: 1.245471\n",
      "Cost after iteration 8700: 1.245451\n",
      "Cost after iteration 8800: 1.245432\n",
      "Cost after iteration 8900: 1.245413\n",
      "Cost after iteration 9000: 1.245394\n",
      "Cost after iteration 9100: 1.245375\n",
      "Cost after iteration 9200: 1.245356\n",
      "Cost after iteration 9300: 1.245337\n",
      "Cost after iteration 9400: 1.245318\n",
      "Cost after iteration 9500: 1.245300\n",
      "Cost after iteration 9600: 1.245281\n",
      "Cost after iteration 9700: 1.245263\n",
      "Cost after iteration 9800: 1.245245\n",
      "Cost after iteration 9900: 1.245227\n",
      "train accuracy: 96.6349371798598 %\n",
      "{'costs': [array(1.29965096), array(1.24785313), array(1.24768376), array(1.24764889), array(1.24761465), array(1.24758064), array(1.24754687), array(1.24751333), array(1.24748001), array(1.24744693), array(1.24741407), array(1.24738144), array(1.24734904), array(1.24731685), array(1.24728488), array(1.24725314), array(1.2472216), array(1.24719029), array(1.24715919), array(1.2471283), array(1.24709762), array(1.24706715), array(1.24703689), array(1.24700684), array(1.24697699), array(1.24694734), array(1.2469179), array(1.24688865), array(1.24685961), array(1.24683076), array(1.24680211), array(1.24677365), array(1.24674539), array(1.24671731), array(1.24668943), array(1.24666174), array(1.24663424), array(1.24660692), array(1.24657978), array(1.24655284), array(1.24652607), array(1.24649948), array(1.24647308), array(1.24644685), array(1.2464208), array(1.24639492), array(1.24636922), array(1.2463437), array(1.24631834), array(1.24629316), array(1.24626814), array(1.2462433), array(1.24621862), array(1.24619411), array(1.24616976), array(1.24614557), array(1.24612155), array(1.24609769), array(1.24607399), array(1.24605045), array(1.24602706), array(1.24600384), array(1.24598076), array(1.24595785), array(1.24593508), array(1.24591247), array(1.24589001), array(1.2458677), array(1.24584554), array(1.24582352), array(1.24580165), array(1.24577993), array(1.24575835), array(1.24573692), array(1.24571563), array(1.24569448), array(1.24567347), array(1.2456526), array(1.24563187), array(1.24561128), array(1.24559082), array(1.2455705), array(1.24555031), array(1.24553025), array(1.24551033), array(1.24549054), array(1.24547088), array(1.24545135), array(1.24543195), array(1.24541268), array(1.24539354), array(1.24537452), array(1.24535562), array(1.24533685), array(1.2453182), array(1.24529968), array(1.24528128), array(1.24526299), array(1.24524483), array(1.24522679)], 'Y_prediction_test': array([[0.62199014, 0.61338395, 0.6186434 , 0.6187071 , 0.61390715,\n",
      "        0.60731866, 0.64112435, 0.61786881, 0.62417805, 0.62362649,\n",
      "        0.61716645, 0.60359276, 0.62814287, 0.622708  , 0.62469699,\n",
      "        0.61582276, 0.61978861, 0.60677066, 0.61841043, 0.6261717 ,\n",
      "        0.61384212, 0.61102906, 0.61544076, 0.62318941, 0.60963608,\n",
      "        0.60805565, 0.6158786 , 0.62432331, 0.62446928, 0.62274454]]), 'Y_prediction_train': array([[0.60807929, 0.606566  , 0.62173367, 0.62715992, 0.61977225,\n",
      "        0.62017199, 0.62243648, 0.62113349, 0.61201482, 0.6225108 ,\n",
      "        0.61199335, 0.60776742, 0.61377941, 0.62337219, 0.62565033,\n",
      "        0.62234962, 0.62074482, 0.62524012, 0.61555113, 0.61399078,\n",
      "        0.62051042, 0.62303645, 0.62240476, 0.62026078, 0.61803921,\n",
      "        0.60765303, 0.62296792, 0.62387909, 0.62202461, 0.62467121,\n",
      "        0.61456192, 0.61298224, 0.60633666, 0.60957502, 0.61723135,\n",
      "        0.60728028, 0.61611259, 0.62129029, 0.62067628, 0.62370979,\n",
      "        0.62421331, 0.58945349, 0.61683297, 0.62176369, 0.61381594,\n",
      "        0.61027624, 0.61794143, 0.60918196, 0.6129869 , 0.62359173,\n",
      "        0.62340267, 0.60945228, 0.6212313 , 0.61592049, 0.62120382,\n",
      "        0.61228445, 0.61040534, 0.62215084, 0.61015787, 0.61025801,\n",
      "        0.60585375, 0.61008233, 0.61168099, 0.62269501, 0.61009136,\n",
      "        0.609972  , 0.62417321, 0.61271757, 0.6223362 , 0.62359501,\n",
      "        0.61715436, 0.61416434, 0.62191888, 0.61537538, 0.60346034,\n",
      "        0.62416838, 0.61854046, 0.64541782, 0.62019387, 0.60629471,\n",
      "        0.62088397, 0.61145173, 0.613115  , 0.61771995, 0.62354058,\n",
      "        0.62558263, 0.62077875, 0.61208042, 0.61554388, 0.62222489,\n",
      "        0.6170031 , 0.6134626 , 0.6227323 , 0.64158243, 0.62314239,\n",
      "        0.62715286, 0.61285607, 0.61827422, 0.61551612, 0.62260396,\n",
      "        0.61674032, 0.62261625, 0.60822536, 0.62297365, 0.61867296,\n",
      "        0.62204518, 0.61677621, 0.60990691, 0.60901385, 0.62214248,\n",
      "        0.60741563, 0.62012603, 0.61254275, 0.60549014, 0.60377895,\n",
      "        0.61491238, 0.63216583, 0.62267799, 0.61767012, 0.62226859]]), 'w': array([[-0.01187773],\n",
      "       [ 0.00105118],\n",
      "       [ 0.03384237],\n",
      "       [-0.0131696 ],\n",
      "       [-0.01278937],\n",
      "       [ 0.00207671],\n",
      "       [ 0.07299973],\n",
      "       [-0.0073427 ],\n",
      "       [-0.01193837],\n",
      "       [ 0.00139365],\n",
      "       [ 0.07021304],\n",
      "       [-0.00792148],\n",
      "       [-0.01415409],\n",
      "       [ 0.00296386],\n",
      "       [ 0.03772029],\n",
      "       [-0.01288467],\n",
      "       [-0.0162689 ],\n",
      "       [ 0.00415628],\n",
      "       [ 0.02378592],\n",
      "       [-0.01618095],\n",
      "       [-0.02031326],\n",
      "       [ 0.00442421],\n",
      "       [ 0.01736416],\n",
      "       [-0.01832845],\n",
      "       [-0.02061268],\n",
      "       [ 0.00386447],\n",
      "       [ 0.00673096],\n",
      "       [-0.02076885],\n",
      "       [-0.02155902],\n",
      "       [ 0.00346736],\n",
      "       [ 0.00018862],\n",
      "       [-0.02161849],\n",
      "       [-0.02173217],\n",
      "       [ 0.00339402],\n",
      "       [-0.00177446],\n",
      "       [-0.02179347],\n",
      "       [-0.02124712],\n",
      "       [ 0.0034057 ],\n",
      "       [-0.0042427 ],\n",
      "       [-0.02198281],\n",
      "       [-0.02006693],\n",
      "       [ 0.0034062 ],\n",
      "       [-0.01655957],\n",
      "       [-0.02263351],\n",
      "       [-0.01912213],\n",
      "       [ 0.00361528],\n",
      "       [-0.0193996 ],\n",
      "       [-0.02275451],\n",
      "       [-0.01980865],\n",
      "       [ 0.00336217],\n",
      "       [-0.01998125],\n",
      "       [-0.02277651],\n",
      "       [-0.02033281],\n",
      "       [ 0.00375766],\n",
      "       [-0.02097507],\n",
      "       [-0.02281939],\n",
      "       [-0.02200122],\n",
      "       [ 0.00477809],\n",
      "       [-0.02190493],\n",
      "       [-0.02285939],\n",
      "       [-0.02233474],\n",
      "       [ 0.005853  ],\n",
      "       [-0.02255718],\n",
      "       [-0.02288661]]), 'b': 0.02295747003966542, 'learning_rate': 0.001, 'num_iterations': 10000}\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model_all = model(train_set_x_orig_all, train_set_y_all, test_set_x_orig_all, num_iterations=10000, learning_rate=0.001, print_cost=True)\n",
    "print(logistic_regression_model_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.299651\n",
      "Cost after iteration 100: 1.260666\n",
      "Cost after iteration 200: 1.260563\n",
      "Cost after iteration 300: 1.260557\n",
      "Cost after iteration 400: 1.260551\n",
      "Cost after iteration 500: 1.260545\n",
      "Cost after iteration 600: 1.260539\n",
      "Cost after iteration 700: 1.260533\n",
      "Cost after iteration 800: 1.260527\n",
      "Cost after iteration 900: 1.260521\n",
      "Cost after iteration 1000: 1.260515\n",
      "Cost after iteration 1100: 1.260509\n",
      "Cost after iteration 1200: 1.260503\n",
      "Cost after iteration 1300: 1.260497\n",
      "Cost after iteration 1400: 1.260492\n",
      "Cost after iteration 1500: 1.260486\n",
      "Cost after iteration 1600: 1.260480\n",
      "Cost after iteration 1700: 1.260474\n",
      "Cost after iteration 1800: 1.260468\n",
      "Cost after iteration 1900: 1.260462\n",
      "Cost after iteration 2000: 1.260456\n",
      "Cost after iteration 2100: 1.260450\n",
      "Cost after iteration 2200: 1.260444\n",
      "Cost after iteration 2300: 1.260438\n",
      "Cost after iteration 2400: 1.260433\n",
      "Cost after iteration 2500: 1.260427\n",
      "Cost after iteration 2600: 1.260421\n",
      "Cost after iteration 2700: 1.260415\n",
      "Cost after iteration 2800: 1.260409\n",
      "Cost after iteration 2900: 1.260403\n",
      "Cost after iteration 3000: 1.260398\n",
      "Cost after iteration 3100: 1.260392\n",
      "Cost after iteration 3200: 1.260386\n",
      "Cost after iteration 3300: 1.260380\n",
      "Cost after iteration 3400: 1.260374\n",
      "Cost after iteration 3500: 1.260369\n",
      "Cost after iteration 3600: 1.260363\n",
      "Cost after iteration 3700: 1.260357\n",
      "Cost after iteration 3800: 1.260351\n",
      "Cost after iteration 3900: 1.260346\n",
      "Cost after iteration 4000: 1.260340\n",
      "Cost after iteration 4100: 1.260334\n",
      "Cost after iteration 4200: 1.260328\n",
      "Cost after iteration 4300: 1.260323\n",
      "Cost after iteration 4400: 1.260317\n",
      "Cost after iteration 4500: 1.260311\n",
      "Cost after iteration 4600: 1.260306\n",
      "Cost after iteration 4700: 1.260300\n",
      "Cost after iteration 4800: 1.260294\n",
      "Cost after iteration 4900: 1.260288\n",
      "Cost after iteration 5000: 1.260283\n",
      "Cost after iteration 5100: 1.260277\n",
      "Cost after iteration 5200: 1.260271\n",
      "Cost after iteration 5300: 1.260266\n",
      "Cost after iteration 5400: 1.260260\n",
      "Cost after iteration 5500: 1.260254\n",
      "Cost after iteration 5600: 1.260249\n",
      "Cost after iteration 5700: 1.260243\n",
      "Cost after iteration 5800: 1.260237\n",
      "Cost after iteration 5900: 1.260232\n",
      "Cost after iteration 6000: 1.260226\n",
      "Cost after iteration 6100: 1.260221\n",
      "Cost after iteration 6200: 1.260215\n",
      "Cost after iteration 6300: 1.260209\n",
      "Cost after iteration 6400: 1.260204\n",
      "Cost after iteration 6500: 1.260198\n",
      "Cost after iteration 6600: 1.260193\n",
      "Cost after iteration 6700: 1.260187\n",
      "Cost after iteration 6800: 1.260181\n",
      "Cost after iteration 6900: 1.260176\n",
      "Cost after iteration 7000: 1.260170\n",
      "Cost after iteration 7100: 1.260165\n",
      "Cost after iteration 7200: 1.260159\n",
      "Cost after iteration 7300: 1.260154\n",
      "Cost after iteration 7400: 1.260148\n",
      "Cost after iteration 7500: 1.260142\n",
      "Cost after iteration 7600: 1.260137\n",
      "Cost after iteration 7700: 1.260131\n",
      "Cost after iteration 7800: 1.260126\n",
      "Cost after iteration 7900: 1.260120\n",
      "Cost after iteration 8000: 1.260115\n",
      "Cost after iteration 8100: 1.260109\n",
      "Cost after iteration 8200: 1.260104\n",
      "Cost after iteration 8300: 1.260098\n",
      "Cost after iteration 8400: 1.260093\n",
      "Cost after iteration 8500: 1.260087\n",
      "Cost after iteration 8600: 1.260082\n",
      "Cost after iteration 8700: 1.260076\n",
      "Cost after iteration 8800: 1.260071\n",
      "Cost after iteration 8900: 1.260065\n",
      "Cost after iteration 9000: 1.260060\n",
      "Cost after iteration 9100: 1.260054\n",
      "Cost after iteration 9200: 1.260049\n",
      "Cost after iteration 9300: 1.260043\n",
      "Cost after iteration 9400: 1.260038\n",
      "Cost after iteration 9500: 1.260033\n",
      "Cost after iteration 9600: 1.260027\n",
      "Cost after iteration 9700: 1.260022\n",
      "Cost after iteration 9800: 1.260016\n",
      "Cost after iteration 9900: 1.260011\n",
      "train accuracy: 93.3656163433279 %\n",
      "{'costs': [array(1.29965096), array(1.26066552), array(1.26056348), array(1.26055721), array(1.2605512), array(1.2605452), array(1.2605392), array(1.26053322), array(1.26052724), array(1.26052127), array(1.26051531), array(1.26050935), array(1.26050341), array(1.26049747), array(1.26049154), array(1.26048561), array(1.2604797), array(1.26047379), array(1.26046789), array(1.260462), array(1.26045611), array(1.26045023), array(1.26044436), array(1.2604385), array(1.26043264), array(1.26042679), array(1.26042095), array(1.26041512), array(1.26040929), array(1.26040347), array(1.26039765), array(1.26039185), array(1.26038605), array(1.26038025), array(1.26037446), array(1.26036868), array(1.26036291), array(1.26035714), array(1.26035138), array(1.26034563), array(1.26033988), array(1.26033414), array(1.26032841), array(1.26032268), array(1.26031696), array(1.26031124), array(1.26030553), array(1.26029983), array(1.26029413), array(1.26028844), array(1.26028275), array(1.26027707), array(1.2602714), array(1.26026573), array(1.26026007), array(1.26025441), array(1.26024876), array(1.26024312), array(1.26023748), array(1.26023185), array(1.26022622), array(1.2602206), array(1.26021498), array(1.26020937), array(1.26020377), array(1.26019817), array(1.26019257), array(1.26018699), array(1.2601814), array(1.26017582), array(1.26017025), array(1.26016468), array(1.26015912), array(1.26015356), array(1.26014801), array(1.26014246), array(1.26013692), array(1.26013139), array(1.26012585), array(1.26012033), array(1.2601148), array(1.26010929), array(1.26010378), array(1.26009827), array(1.26009277), array(1.26008727), array(1.26008177), array(1.26007629), array(1.2600708), array(1.26006532), array(1.26005985), array(1.26005438), array(1.26004891), array(1.26004345), array(1.260038), array(1.26003255), array(1.2600271), array(1.26002166), array(1.26001622), array(1.26001078)], 'Y_prediction_test': array([[0.60231671, 0.60197621, 0.60249994, 0.60418913, 0.60344351,\n",
      "        0.6036359 , 0.59838592, 0.60047283, 0.60081731, 0.60056932,\n",
      "        0.60354501, 0.60504248, 0.60101722, 0.60040343, 0.60051638,\n",
      "        0.60435098, 0.60183831, 0.60405434, 0.59948038, 0.59970989,\n",
      "        0.6049539 , 0.6041023 , 0.60285573, 0.60065932, 0.60484776,\n",
      "        0.6051382 , 0.60245398, 0.60066215, 0.60069459, 0.60212246]]), 'Y_prediction_train': array([[0.6022062 , 0.60576317, 0.59965901, 0.60019426, 0.60123645,\n",
      "        0.60002096, 0.6005126 , 0.60091482, 0.60456529, 0.60088215,\n",
      "        0.60409084, 0.60399993, 0.60260151, 0.59902966, 0.60011282,\n",
      "        0.60057289, 0.59936456, 0.59969572, 0.60229939, 0.60419675,\n",
      "        0.60073988, 0.59976421, 0.60047393, 0.60150924, 0.60193361,\n",
      "        0.60466796, 0.60035126, 0.59997307, 0.6002365 , 0.59938181,\n",
      "        0.60111846, 0.6035544 , 0.60472548, 0.60469814, 0.60282556,\n",
      "        0.60314782, 0.60224728, 0.60010094, 0.59968417, 0.60060175,\n",
      "        0.5998288 , 0.60471733, 0.60116818, 0.60067052, 0.60367715,\n",
      "        0.60184737, 0.6019498 , 0.60277513, 0.60340688, 0.59965547,\n",
      "        0.59991085, 0.60499834, 0.60477756, 0.60158002, 0.6007464 ,\n",
      "        0.6024487 , 0.6030569 , 0.59963582, 0.60476223, 0.60268453,\n",
      "        0.60429728, 0.60409856, 0.60352757, 0.5996029 , 0.60334841,\n",
      "        0.60366757, 0.59886038, 0.60349012, 0.59982734, 0.59974414,\n",
      "        0.6019521 , 0.60144182, 0.60070274, 0.60094148, 0.60370807,\n",
      "        0.60000091, 0.60176649, 0.59837306, 0.60088914, 0.60294169,\n",
      "        0.60126768, 0.60526429, 0.60261185, 0.60242653, 0.59931483,\n",
      "        0.59952972, 0.60119025, 0.60379304, 0.60147237, 0.60017067,\n",
      "        0.60090293, 0.60355361, 0.59984123, 0.59994842, 0.59984414,\n",
      "        0.59871825, 0.60143382, 0.59938122, 0.60296687, 0.60033294,\n",
      "        0.60165092, 0.59975667, 0.60346837, 0.59978603, 0.60033523,\n",
      "        0.59965382, 0.60187196, 0.6045188 , 0.60191826, 0.59963861,\n",
      "        0.60552108, 0.60114693, 0.60307584, 0.60426542, 0.60450116,\n",
      "        0.602641  , 0.60454535, 0.60058383, 0.60252793, 0.60055395]]), 'w': array([[ 0.01505772],\n",
      "       [-0.00871604],\n",
      "       [ 0.00900414],\n",
      "       [-0.00799558],\n",
      "       [ 0.018448  ],\n",
      "       [-0.01003812],\n",
      "       [ 0.01601241],\n",
      "       [-0.00813955],\n",
      "       [ 0.01902514],\n",
      "       [-0.01001583],\n",
      "       [ 0.01595116],\n",
      "       [-0.00820383],\n",
      "       [ 0.01886852],\n",
      "       [-0.00968196],\n",
      "       [ 0.00883893],\n",
      "       [-0.00838849],\n",
      "       [ 0.01797385],\n",
      "       [-0.00921041],\n",
      "       [ 0.0030443 ],\n",
      "       [-0.00858484],\n",
      "       [ 0.01717895],\n",
      "       [-0.00904298],\n",
      "       [-0.00170519],\n",
      "       [-0.00893744],\n",
      "       [ 0.01847301],\n",
      "       [-0.00916278],\n",
      "       [-0.00649986],\n",
      "       [-0.00946797],\n",
      "       [ 0.01641794],\n",
      "       [-0.0085339 ],\n",
      "       [-0.00780259],\n",
      "       [-0.00963986],\n",
      "       [ 0.00907632],\n",
      "       [-0.00668644],\n",
      "       [-0.00698442],\n",
      "       [-0.00960687],\n",
      "       [ 0.00450266],\n",
      "       [-0.00568958],\n",
      "       [-0.00688009],\n",
      "       [-0.00962581],\n",
      "       [-0.00256271],\n",
      "       [-0.00371056],\n",
      "       [-0.00854181],\n",
      "       [-0.00984562],\n",
      "       [-0.00341644],\n",
      "       [-0.00348459],\n",
      "       [-0.00913689],\n",
      "       [-0.00989608],\n",
      "       [-0.01155673],\n",
      "       [-0.00232303],\n",
      "       [-0.00913356],\n",
      "       [-0.00989838],\n",
      "       [-0.01681451],\n",
      "       [-0.00183769],\n",
      "       [-0.00938277],\n",
      "       [-0.00991612],\n",
      "       [-0.02719751],\n",
      "       [-0.0005001 ],\n",
      "       [-0.00962071],\n",
      "       [-0.00993374],\n",
      "       [-0.02843501],\n",
      "       [ 0.00029926],\n",
      "       [-0.00983569],\n",
      "       [-0.00994948]]), 'b': 0.011503541854245188, 'learning_rate': 0.001, 'num_iterations': 10000}\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model_home = model(train_set_x_orig_home, train_set_y_home, test_set_x_orig_home, num_iterations=10000, learning_rate=0.001, print_cost=True)\n",
    "print(logistic_regression_model_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.299651\n",
      "Cost after iteration 100: 1.143577\n",
      "Cost after iteration 200: 1.140238\n",
      "Cost after iteration 300: 1.137850\n",
      "Cost after iteration 400: 1.135698\n",
      "Cost after iteration 500: 1.133753\n",
      "Cost after iteration 600: 1.131996\n",
      "Cost after iteration 700: 1.130409\n",
      "Cost after iteration 800: 1.128973\n",
      "Cost after iteration 900: 1.127675\n",
      "Cost after iteration 1000: 1.126501\n",
      "Cost after iteration 1100: 1.125438\n",
      "Cost after iteration 1200: 1.124476\n",
      "Cost after iteration 1300: 1.123605\n",
      "Cost after iteration 1400: 1.122815\n",
      "Cost after iteration 1500: 1.122100\n",
      "Cost after iteration 1600: 1.121451\n",
      "Cost after iteration 1700: 1.120863\n",
      "Cost after iteration 1800: 1.120329\n",
      "Cost after iteration 1900: 1.119844\n",
      "Cost after iteration 2000: 1.119404\n",
      "Cost after iteration 2100: 1.119003\n",
      "Cost after iteration 2200: 1.118639\n",
      "Cost after iteration 2300: 1.118308\n",
      "Cost after iteration 2400: 1.118007\n",
      "Cost after iteration 2500: 1.117732\n",
      "Cost after iteration 2600: 1.117482\n",
      "Cost after iteration 2700: 1.117253\n",
      "Cost after iteration 2800: 1.117045\n",
      "Cost after iteration 2900: 1.116855\n",
      "Cost after iteration 3000: 1.116681\n",
      "Cost after iteration 3100: 1.116521\n",
      "Cost after iteration 3200: 1.116375\n",
      "Cost after iteration 3300: 1.116242\n",
      "Cost after iteration 3400: 1.116119\n",
      "Cost after iteration 3500: 1.116006\n",
      "Cost after iteration 3600: 1.115903\n",
      "Cost after iteration 3700: 1.115807\n",
      "Cost after iteration 3800: 1.115719\n",
      "Cost after iteration 3900: 1.115638\n",
      "Cost after iteration 4000: 1.115563\n",
      "Cost after iteration 4100: 1.115493\n",
      "Cost after iteration 4200: 1.115429\n",
      "Cost after iteration 4300: 1.115369\n",
      "Cost after iteration 4400: 1.115313\n",
      "Cost after iteration 4500: 1.115261\n",
      "Cost after iteration 4600: 1.115213\n",
      "Cost after iteration 4700: 1.115167\n",
      "Cost after iteration 4800: 1.115125\n",
      "Cost after iteration 4900: 1.115085\n",
      "Cost after iteration 5000: 1.115047\n",
      "Cost after iteration 5100: 1.115012\n",
      "Cost after iteration 5200: 1.114978\n",
      "Cost after iteration 5300: 1.114947\n",
      "Cost after iteration 5400: 1.114916\n",
      "Cost after iteration 5500: 1.114888\n",
      "Cost after iteration 5600: 1.114860\n",
      "Cost after iteration 5700: 1.114834\n",
      "Cost after iteration 5800: 1.114809\n",
      "Cost after iteration 5900: 1.114785\n",
      "Cost after iteration 6000: 1.114762\n",
      "Cost after iteration 6100: 1.114740\n",
      "Cost after iteration 6200: 1.114718\n",
      "Cost after iteration 6300: 1.114698\n",
      "Cost after iteration 6400: 1.114677\n",
      "Cost after iteration 6500: 1.114658\n",
      "Cost after iteration 6600: 1.114639\n",
      "Cost after iteration 6700: 1.114620\n",
      "Cost after iteration 6800: 1.114602\n",
      "Cost after iteration 6900: 1.114584\n",
      "Cost after iteration 7000: 1.114567\n",
      "Cost after iteration 7100: 1.114550\n",
      "Cost after iteration 7200: 1.114533\n",
      "Cost after iteration 7300: 1.114517\n",
      "Cost after iteration 7400: 1.114501\n",
      "Cost after iteration 7500: 1.114485\n",
      "Cost after iteration 7600: 1.114469\n",
      "Cost after iteration 7700: 1.114454\n",
      "Cost after iteration 7800: 1.114439\n",
      "Cost after iteration 7900: 1.114423\n",
      "Cost after iteration 8000: 1.114409\n",
      "Cost after iteration 8100: 1.114394\n",
      "Cost after iteration 8200: 1.114379\n",
      "Cost after iteration 8300: 1.114365\n",
      "Cost after iteration 8400: 1.114350\n",
      "Cost after iteration 8500: 1.114336\n",
      "Cost after iteration 8600: 1.114322\n",
      "Cost after iteration 8700: 1.114308\n",
      "Cost after iteration 8800: 1.114294\n",
      "Cost after iteration 8900: 1.114280\n",
      "Cost after iteration 9000: 1.114266\n",
      "Cost after iteration 9100: 1.114252\n",
      "Cost after iteration 9200: 1.114238\n",
      "Cost after iteration 9300: 1.114225\n",
      "Cost after iteration 9400: 1.114211\n",
      "Cost after iteration 9500: 1.114198\n",
      "Cost after iteration 9600: 1.114184\n",
      "Cost after iteration 9700: 1.114171\n",
      "Cost after iteration 9800: 1.114158\n",
      "Cost after iteration 9900: 1.114144\n",
      "train accuracy: 95.86084712620115 %\n",
      "{'costs': [array(1.29965096), array(1.14357674), array(1.14023834), array(1.13785027), array(1.13569761), array(1.13375318), array(1.13199635), array(1.13040857), array(1.12897314), array(1.12767504), array(1.12650072), array(1.12543802), array(1.12447597), array(1.12360473), array(1.1228154), array(1.1221), array(1.12145134), array(1.12086293), array(1.12032894), array(1.1198441), array(1.11940368), array(1.11900341), array(1.11863943), array(1.11830826), array(1.11800678), array(1.11773215), array(1.11748182), array(1.11725349), array(1.11704506), array(1.11685467), array(1.11668061), array(1.11652134), array(1.11637548), array(1.11624176), array(1.11611906), array(1.11600635), array(1.11590268), array(1.11580723), array(1.11571923), array(1.11563799), array(1.11556289), array(1.11549335), array(1.11542886), array(1.11536897), array(1.11531325), array(1.11526131), array(1.11521281), array(1.11516744), array(1.11512491), array(1.11508496), array(1.11504736), array(1.1150119), array(1.11497838), array(1.11494662), array(1.11491648), array(1.1148878), array(1.11486045), array(1.11483432), array(1.11480929), array(1.11478527), array(1.11476217), array(1.11473991), array(1.11471841), array(1.11469761), array(1.11467744), array(1.11465786), array(1.11463881), array(1.11462024), array(1.11460212), array(1.11458441), array(1.11456707), array(1.11455007), array(1.11453338), array(1.11451698), array(1.11450085), array(1.11448496), array(1.11446929), array(1.11445382), array(1.11443855), array(1.11442345), array(1.11440852), array(1.11439373), array(1.11437908), array(1.11436457), array(1.11435017), array(1.11433588), array(1.11432169), array(1.11430761), array(1.11429361), array(1.1142797), array(1.11426587), array(1.11425211), array(1.11423842), array(1.1142248), array(1.11421125), array(1.11419775), array(1.11418431), array(1.11417092), array(1.11415759), array(1.1141443)], 'Y_prediction_test': array([[0.78665614, 0.77084392, 0.6747918 , 0.66068473, 0.7271224 ,\n",
      "        0.77576367, 0.54929192, 0.57532026, 0.62946384, 0.56811006,\n",
      "        0.71314918, 0.80662977, 0.7856589 , 0.57416979, 0.59094892,\n",
      "        0.80447038, 0.74490128, 0.8022317 , 0.81933191, 0.58486879,\n",
      "        0.79781052, 0.81577493, 0.68569977, 0.57435602, 0.81423588,\n",
      "        0.81387045, 0.68598957, 0.57271909, 0.57046401, 0.58299991]]), 'Y_prediction_train': array([[0.82834408, 0.84014312, 0.59750926, 0.79797506, 0.63984247,\n",
      "        0.62480931, 0.57696334, 0.6780323 , 0.83759791, 0.55798309,\n",
      "        0.81448945, 0.83323317, 0.72899128, 0.56350792, 0.7583197 ,\n",
      "        0.65635896, 0.57430399, 0.59233062, 0.74709766, 0.71580518,\n",
      "        0.6372195 , 0.58992258, 0.68961031, 0.71565359, 0.75629974,\n",
      "        0.83241039, 0.58030449, 0.57643543, 0.6133932 , 0.59958731,\n",
      "        0.81857059, 0.82300117, 0.8292476 , 0.8105039 , 0.65307134,\n",
      "        0.80284257, 0.74564858, 0.63208955, 0.58520465, 0.59398704,\n",
      "        0.56494921, 0.7255308 , 0.73686919, 0.5800858 , 0.69338013,\n",
      "        0.70229524, 0.69266751, 0.73690413, 0.7983264 , 0.57229538,\n",
      "        0.58001462, 0.81336815, 0.58296536, 0.70040384, 0.58694848,\n",
      "        0.78930438, 0.67083632, 0.59911855, 0.79095866, 0.8283771 ,\n",
      "        0.83342533, 0.81965419, 0.82074473, 0.57319338, 0.77213507,\n",
      "        0.7006793 , 0.57279954, 0.75795069, 0.57289616, 0.56768958,\n",
      "        0.68834307, 0.69293602, 0.59593415, 0.67832179, 0.80008919,\n",
      "        0.59776182, 0.64606625, 0.48452192, 0.6576833 , 0.77100716,\n",
      "        0.67711724, 0.81599324, 0.74136011, 0.74049569, 0.5733965 ,\n",
      "        0.72866569, 0.75896416, 0.80593817, 0.7361239 , 0.62819061,\n",
      "        0.62115334, 0.75397729, 0.58226822, 0.52533678, 0.57813232,\n",
      "        0.59422575, 0.67466366, 0.59844085, 0.81033373, 0.76513387,\n",
      "        0.68260724, 0.58388328, 0.84118447, 0.5685076 , 0.6907962 ,\n",
      "        0.59174865, 0.73707357, 0.83159941, 0.80385758, 0.60020954,\n",
      "        0.83253571, 0.67520876, 0.79322896, 0.84760663, 0.81995497,\n",
      "        0.73009893, 0.55496656, 0.58514343, 0.72879669, 0.5961816 ]]), 'w': array([[ 0.08266534],\n",
      "       [-0.02362853],\n",
      "       [-0.12409492],\n",
      "       [-0.0093008 ],\n",
      "       [ 0.08572847],\n",
      "       [-0.02582537],\n",
      "       [-0.18115301],\n",
      "       [-0.01114652],\n",
      "       [ 0.08456955],\n",
      "       [-0.02848324],\n",
      "       [-0.19433152],\n",
      "       [-0.01425856],\n",
      "       [ 0.06924025],\n",
      "       [-0.02526523],\n",
      "       [-0.13422731],\n",
      "       [-0.01307992],\n",
      "       [ 0.0548109 ],\n",
      "       [-0.02282937],\n",
      "       [-0.07727269],\n",
      "       [-0.01127127],\n",
      "       [ 0.03176062],\n",
      "       [-0.01463857],\n",
      "       [-0.032073  ],\n",
      "       [-0.0082692 ],\n",
      "       [ 0.02365068],\n",
      "       [-0.01090328],\n",
      "       [-0.01264186],\n",
      "       [-0.00635526],\n",
      "       [ 0.01371259],\n",
      "       [-0.00707371],\n",
      "       [-0.00721404],\n",
      "       [-0.00577012],\n",
      "       [ 0.00830625],\n",
      "       [-0.00501274],\n",
      "       [-0.00540555],\n",
      "       [-0.00563351],\n",
      "       [ 0.00296224],\n",
      "       [-0.00342737],\n",
      "       [-0.00434245],\n",
      "       [-0.00554808],\n",
      "       [ 0.00089393],\n",
      "       [-0.00315987],\n",
      "       [-0.00506774],\n",
      "       [-0.00559003],\n",
      "       [-0.00378331],\n",
      "       [-0.00234335],\n",
      "       [-0.00520291],\n",
      "       [-0.00559776],\n",
      "       [-0.00345468],\n",
      "       [-0.00216728],\n",
      "       [-0.00524663],\n",
      "       [-0.00560252],\n",
      "       [-0.00444386],\n",
      "       [-0.00188136],\n",
      "       [-0.00535513],\n",
      "       [-0.00560734],\n",
      "       [-0.00899923],\n",
      "       [-0.00070552],\n",
      "       [-0.00544539],\n",
      "       [-0.00561182],\n",
      "       [-0.01556071],\n",
      "       [ 0.00187538],\n",
      "       [-0.00553147],\n",
      "       [-0.00561561]]), 'b': 0.007223019315863578, 'learning_rate': 0.001, 'num_iterations': 10000}\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model_others = model(train_set_x_orig_others, train_set_y_others, test_set_x_orig_others, num_iterations=10000, learning_rate=0.001, print_cost=True)\n",
    "print(logistic_regression_model_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.299651\n",
      "Cost after iteration 100: 1.299253\n",
      "Cost after iteration 200: 1.299252\n",
      "Cost after iteration 300: 1.299252\n",
      "Cost after iteration 400: 1.299252\n",
      "Cost after iteration 500: 1.299252\n",
      "Cost after iteration 600: 1.299252\n",
      "Cost after iteration 700: 1.299252\n",
      "Cost after iteration 800: 1.299252\n",
      "Cost after iteration 900: 1.299252\n",
      "Cost after iteration 1000: 1.299252\n",
      "Cost after iteration 1100: 1.299252\n",
      "Cost after iteration 1200: 1.299252\n",
      "Cost after iteration 1300: 1.299252\n",
      "Cost after iteration 1400: 1.299252\n",
      "Cost after iteration 1500: 1.299252\n",
      "Cost after iteration 1600: 1.299252\n",
      "Cost after iteration 1700: 1.299252\n",
      "Cost after iteration 1800: 1.299252\n",
      "Cost after iteration 1900: 1.299252\n",
      "Cost after iteration 2000: 1.299252\n",
      "Cost after iteration 2100: 1.299252\n",
      "Cost after iteration 2200: 1.299252\n",
      "Cost after iteration 2300: 1.299252\n",
      "Cost after iteration 2400: 1.299252\n",
      "Cost after iteration 2500: 1.299252\n",
      "Cost after iteration 2600: 1.299252\n",
      "Cost after iteration 2700: 1.299252\n",
      "Cost after iteration 2800: 1.299252\n",
      "Cost after iteration 2900: 1.299252\n",
      "Cost after iteration 3000: 1.299252\n",
      "Cost after iteration 3100: 1.299252\n",
      "Cost after iteration 3200: 1.299252\n",
      "Cost after iteration 3300: 1.299252\n",
      "Cost after iteration 3400: 1.299252\n",
      "Cost after iteration 3500: 1.299252\n",
      "Cost after iteration 3600: 1.299252\n",
      "Cost after iteration 3700: 1.299252\n",
      "Cost after iteration 3800: 1.299252\n",
      "Cost after iteration 3900: 1.299252\n",
      "Cost after iteration 4000: 1.299252\n",
      "Cost after iteration 4100: 1.299252\n",
      "Cost after iteration 4200: 1.299252\n",
      "Cost after iteration 4300: 1.299252\n",
      "Cost after iteration 4400: 1.299252\n",
      "Cost after iteration 4500: 1.299252\n",
      "Cost after iteration 4600: 1.299252\n",
      "Cost after iteration 4700: 1.299252\n",
      "Cost after iteration 4800: 1.299252\n",
      "Cost after iteration 4900: 1.299252\n",
      "Cost after iteration 5000: 1.299252\n",
      "Cost after iteration 5100: 1.299252\n",
      "Cost after iteration 5200: 1.299252\n",
      "Cost after iteration 5300: 1.299252\n",
      "Cost after iteration 5400: 1.299252\n",
      "Cost after iteration 5500: 1.299252\n",
      "Cost after iteration 5600: 1.299252\n",
      "Cost after iteration 5700: 1.299252\n",
      "Cost after iteration 5800: 1.299252\n",
      "Cost after iteration 5900: 1.299252\n",
      "Cost after iteration 6000: 1.299252\n",
      "Cost after iteration 6100: 1.299252\n",
      "Cost after iteration 6200: 1.299252\n",
      "Cost after iteration 6300: 1.299252\n",
      "Cost after iteration 6400: 1.299252\n",
      "Cost after iteration 6500: 1.299252\n",
      "Cost after iteration 6600: 1.299252\n",
      "Cost after iteration 6700: 1.299252\n",
      "Cost after iteration 6800: 1.299252\n",
      "Cost after iteration 6900: 1.299252\n",
      "Cost after iteration 7000: 1.299252\n",
      "Cost after iteration 7100: 1.299252\n",
      "Cost after iteration 7200: 1.299252\n",
      "Cost after iteration 7300: 1.299252\n",
      "Cost after iteration 7400: 1.299252\n",
      "Cost after iteration 7500: 1.299252\n",
      "Cost after iteration 7600: 1.299252\n",
      "Cost after iteration 7700: 1.299252\n",
      "Cost after iteration 7800: 1.299252\n",
      "Cost after iteration 7900: 1.299252\n",
      "Cost after iteration 8000: 1.299252\n",
      "Cost after iteration 8100: 1.299252\n",
      "Cost after iteration 8200: 1.299252\n",
      "Cost after iteration 8300: 1.299252\n",
      "Cost after iteration 8400: 1.299252\n",
      "Cost after iteration 8500: 1.299252\n",
      "Cost after iteration 8600: 1.299252\n",
      "Cost after iteration 8700: 1.299252\n",
      "Cost after iteration 8800: 1.299252\n",
      "Cost after iteration 8900: 1.299252\n",
      "Cost after iteration 9000: 1.299252\n",
      "Cost after iteration 9100: 1.299252\n",
      "Cost after iteration 9200: 1.299252\n",
      "Cost after iteration 9300: 1.299252\n",
      "Cost after iteration 9400: 1.299252\n",
      "Cost after iteration 9500: 1.299252\n",
      "Cost after iteration 9600: 1.299252\n",
      "Cost after iteration 9700: 1.299252\n",
      "Cost after iteration 9800: 1.299252\n",
      "Cost after iteration 9900: 1.299252\n",
      "train accuracy: 88.30555563729378 %\n",
      "{'costs': [array(1.29965096), array(1.29925274), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192), array(1.29925192)], 'Y_prediction_test': array([[0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476]]), 'Y_prediction_train': array([[0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476,\n",
      "        0.48968476, 0.48968476, 0.48968476, 0.48968476, 0.48968476]]), 'w': array([[-0.00109963],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109963],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109963],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109963],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109963],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109963],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109964],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109964],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109964],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109963],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109964],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109964],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109967],\n",
      "       [ 0.00036655],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109966],\n",
      "       [ 0.00036655],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109964],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [-0.00109964],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654],\n",
      "       [ 0.00036654]]), 'b': -0.0006348693830851725, 'learning_rate': 0.001, 'num_iterations': 10000}\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model_school = model(train_set_x_orig_school, train_set_y_school, test_set_x_orig_school, num_iterations=10000, learning_rate=0.001, print_cost=True)\n",
    "print(logistic_regression_model_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.299651\n",
      "Cost after iteration 100: 1.299029\n",
      "Cost after iteration 200: 1.298449\n",
      "Cost after iteration 300: 1.297881\n",
      "Cost after iteration 400: 1.297326\n",
      "Cost after iteration 500: 1.296783\n",
      "Cost after iteration 600: 1.296252\n",
      "Cost after iteration 700: 1.295732\n",
      "Cost after iteration 800: 1.295223\n",
      "Cost after iteration 900: 1.294725\n",
      "Cost after iteration 1000: 1.294238\n",
      "Cost after iteration 1100: 1.293761\n",
      "Cost after iteration 1200: 1.293295\n",
      "Cost after iteration 1300: 1.292838\n",
      "Cost after iteration 1400: 1.292392\n",
      "Cost after iteration 1500: 1.291954\n",
      "Cost after iteration 1600: 1.291527\n",
      "Cost after iteration 1700: 1.291108\n",
      "Cost after iteration 1800: 1.290699\n",
      "Cost after iteration 1900: 1.290298\n",
      "Cost after iteration 2000: 1.289906\n",
      "Cost after iteration 2100: 1.289522\n",
      "Cost after iteration 2200: 1.289146\n",
      "Cost after iteration 2300: 1.288779\n",
      "Cost after iteration 2400: 1.288419\n",
      "Cost after iteration 2500: 1.288067\n",
      "Cost after iteration 2600: 1.287722\n",
      "Cost after iteration 2700: 1.287384\n",
      "Cost after iteration 2800: 1.287054\n",
      "Cost after iteration 2900: 1.286731\n",
      "Cost after iteration 3000: 1.286415\n",
      "Cost after iteration 3100: 1.286105\n",
      "Cost after iteration 3200: 1.285802\n",
      "Cost after iteration 3300: 1.285505\n",
      "Cost after iteration 3400: 1.285214\n",
      "Cost after iteration 3500: 1.284930\n",
      "Cost after iteration 3600: 1.284652\n",
      "Cost after iteration 3700: 1.284379\n",
      "Cost after iteration 3800: 1.284112\n",
      "Cost after iteration 3900: 1.283851\n",
      "Cost after iteration 4000: 1.283595\n",
      "Cost after iteration 4100: 1.283345\n",
      "Cost after iteration 4200: 1.283099\n",
      "Cost after iteration 4300: 1.282859\n",
      "Cost after iteration 4400: 1.282624\n",
      "Cost after iteration 4500: 1.282394\n",
      "Cost after iteration 4600: 1.282168\n",
      "Cost after iteration 4700: 1.281947\n",
      "Cost after iteration 4800: 1.281731\n",
      "Cost after iteration 4900: 1.281519\n",
      "Cost after iteration 5000: 1.281312\n",
      "Cost after iteration 5100: 1.281109\n",
      "Cost after iteration 5200: 1.280910\n",
      "Cost after iteration 5300: 1.280715\n",
      "Cost after iteration 5400: 1.280524\n",
      "Cost after iteration 5500: 1.280337\n",
      "Cost after iteration 5600: 1.280154\n",
      "Cost after iteration 5700: 1.279975\n",
      "Cost after iteration 5800: 1.279799\n",
      "Cost after iteration 5900: 1.279627\n",
      "Cost after iteration 6000: 1.279458\n",
      "Cost after iteration 6100: 1.279293\n",
      "Cost after iteration 6200: 1.279131\n",
      "Cost after iteration 6300: 1.278972\n",
      "Cost after iteration 6400: 1.278817\n",
      "Cost after iteration 6500: 1.278665\n",
      "Cost after iteration 6600: 1.278515\n",
      "Cost after iteration 6700: 1.278369\n",
      "Cost after iteration 6800: 1.278226\n",
      "Cost after iteration 6900: 1.278085\n",
      "Cost after iteration 7000: 1.277948\n",
      "Cost after iteration 7100: 1.277813\n",
      "Cost after iteration 7200: 1.277681\n",
      "Cost after iteration 7300: 1.277551\n",
      "Cost after iteration 7400: 1.277424\n",
      "Cost after iteration 7500: 1.277300\n",
      "Cost after iteration 7600: 1.277178\n",
      "Cost after iteration 7700: 1.277058\n",
      "Cost after iteration 7800: 1.276941\n",
      "Cost after iteration 7900: 1.276826\n",
      "Cost after iteration 8000: 1.276713\n",
      "Cost after iteration 8100: 1.276603\n",
      "Cost after iteration 8200: 1.276494\n",
      "Cost after iteration 8300: 1.276388\n",
      "Cost after iteration 8400: 1.276284\n",
      "Cost after iteration 8500: 1.276182\n",
      "Cost after iteration 8600: 1.276082\n",
      "Cost after iteration 8700: 1.275983\n",
      "Cost after iteration 8800: 1.275887\n",
      "Cost after iteration 8900: 1.275793\n",
      "Cost after iteration 9000: 1.275700\n",
      "Cost after iteration 9100: 1.275609\n",
      "Cost after iteration 9200: 1.275520\n",
      "Cost after iteration 9300: 1.275433\n",
      "Cost after iteration 9400: 1.275347\n",
      "Cost after iteration 9500: 1.275263\n",
      "Cost after iteration 9600: 1.275180\n",
      "Cost after iteration 9700: 1.275099\n",
      "Cost after iteration 9800: 1.275020\n",
      "Cost after iteration 9900: 1.274942\n",
      "train accuracy: 97.5638119368956 %\n",
      "{'costs': [array(1.29965096), array(1.29902882), array(1.29844883), array(1.29788141), array(1.29732624), array(1.29678304), array(1.29625156), array(1.29573153), array(1.2952227), array(1.29472483), array(1.29423768), array(1.293761), array(1.29329456), array(1.29283815), array(1.29239153), array(1.29195449), array(1.29152682), array(1.29110831), array(1.29069876), array(1.29029797), array(1.28990574), array(1.28952188), array(1.28914621), array(1.28877854), array(1.28841871), array(1.28806652), array(1.28772182), array(1.28738444), array(1.28705421), array(1.28673097), array(1.28641458), array(1.28610487), array(1.2858017), array(1.28550492), array(1.2852144), array(1.28492999), array(1.28465156), array(1.28437897), array(1.28411209), array(1.28385081), array(1.28359499), array(1.28334451), array(1.28309925), array(1.28285911), array(1.28262396), array(1.28239369), array(1.28216821), array(1.28194739), array(1.28173114), array(1.28151936), array(1.28131196), array(1.28110882), array(1.28090986), array(1.28071499), array(1.28052412), array(1.28033715), array(1.28015401), array(1.27997461), array(1.27979886), array(1.27962669), array(1.27945802), array(1.27929278), array(1.27913088), array(1.27897225), array(1.27881683), array(1.27866454), array(1.27851531), array(1.27836908), array(1.27822578), array(1.27808536), array(1.27794773), array(1.27781286), array(1.27768067), array(1.27755111), array(1.27742411), array(1.27729964), array(1.27717762), array(1.27705801), array(1.27694076), array(1.27682581), array(1.27671312), array(1.27660263), array(1.27649431), array(1.27638809), array(1.27628394), array(1.27618182), array(1.27608168), array(1.27598347), array(1.27588716), array(1.27579271), array(1.27570007), array(1.27560921), array(1.27552008), array(1.27543266), array(1.27534691), array(1.27526279), array(1.27518026), array(1.2750993), array(1.27501986), array(1.27494192)], 'Y_prediction_test': array([[0.51123981, 0.41868786, 0.51996984, 0.54181042, 0.48027959,\n",
      "        0.37283348, 0.63428949, 0.4296345 , 0.55771646, 0.54270234,\n",
      "        0.53135303, 0.37616649, 0.55891769, 0.51421645, 0.54414758,\n",
      "        0.56574476, 0.45034315, 0.42158781, 0.39682604, 0.54002307,\n",
      "        0.56160043, 0.48861739, 0.48274171, 0.51601351, 0.52459735,\n",
      "        0.489405  , 0.47527384, 0.52432018, 0.54698754, 0.55245264]]), 'Y_prediction_train': array([[0.39749108, 0.55158355, 0.50824133, 0.54082626, 0.51454259,\n",
      "        0.47562823, 0.54134201, 0.52854205, 0.5649463 , 0.44787986,\n",
      "        0.54209258, 0.50557276, 0.46396086, 0.51068465, 0.52373618,\n",
      "        0.55295192, 0.47046991, 0.56165607, 0.50434457, 0.52648994,\n",
      "        0.5131948 , 0.54404321, 0.52109174, 0.52390834, 0.53548081,\n",
      "        0.53097653, 0.53809008, 0.55228325, 0.50765025, 0.53919798,\n",
      "        0.4740392 , 0.54478635, 0.50971456, 0.52925052, 0.52836895,\n",
      "        0.39658331, 0.49415523, 0.50816882, 0.48857141, 0.53640229,\n",
      "        0.53086918, 0.36482736, 0.4932261 , 0.53394119, 0.51066833,\n",
      "        0.35803653, 0.51733005, 0.39957696, 0.5443794 , 0.53059042,\n",
      "        0.5398614 , 0.5499618 , 0.48065384, 0.47407766, 0.53453384,\n",
      "        0.46232259, 0.50062741, 0.51525733, 0.52081327, 0.50022764,\n",
      "        0.47897021, 0.52942543, 0.49872161, 0.52044165, 0.47317417,\n",
      "        0.453824  , 0.52463963, 0.50460706, 0.51908666, 0.52566397,\n",
      "        0.50376754, 0.41864566, 0.524105  , 0.37340077, 0.36516796,\n",
      "        0.55488465, 0.52186693, 0.62692958, 0.52384775, 0.35117772,\n",
      "        0.54825451, 0.55248036, 0.483297  , 0.55916874, 0.49627258,\n",
      "        0.51496736, 0.55984022, 0.54402594, 0.43721713, 0.53138634,\n",
      "        0.45350744, 0.5201081 , 0.54089975, 0.70292994, 0.5413268 ,\n",
      "        0.54817105, 0.38809233, 0.35000331, 0.56795347, 0.4995621 ,\n",
      "        0.48330202, 0.50011815, 0.5189841 , 0.53373968, 0.49369209,\n",
      "        0.50516112, 0.49088971, 0.54604811, 0.39465353, 0.51076775,\n",
      "        0.54417901, 0.51724443, 0.49514498, 0.51617022, 0.44374672,\n",
      "        0.5019765 , 0.64146232, 0.53360344, 0.55959004, 0.54113145]]), 'w': array([[ 0.04568025],\n",
      "       [-0.04127104],\n",
      "       [ 0.04032866],\n",
      "       [-0.04439749],\n",
      "       [-0.03007869],\n",
      "       [-0.02900449],\n",
      "       [ 0.22249068],\n",
      "       [-0.02847597],\n",
      "       [-0.03192298],\n",
      "       [-0.03248499],\n",
      "       [ 0.19374196],\n",
      "       [-0.028157  ],\n",
      "       [-0.03054925],\n",
      "       [-0.03141974],\n",
      "       [ 0.10105144],\n",
      "       [-0.03194588],\n",
      "       [-0.06684413],\n",
      "       [-0.02067701],\n",
      "       [ 0.07226218],\n",
      "       [-0.03420576],\n",
      "       [-0.07082714],\n",
      "       [-0.0183328 ],\n",
      "       [ 0.02684287],\n",
      "       [-0.04084969],\n",
      "       [-0.06924315],\n",
      "       [-0.01882865],\n",
      "       [-0.00725558],\n",
      "       [-0.04878396],\n",
      "       [-0.06996553],\n",
      "       [-0.01886331],\n",
      "       [-0.02194554],\n",
      "       [-0.05149173],\n",
      "       [-0.06851874],\n",
      "       [-0.01919744],\n",
      "       [-0.02560874],\n",
      "       [-0.05202436],\n",
      "       [-0.06424165],\n",
      "       [-0.02085169],\n",
      "       [-0.03055428],\n",
      "       [-0.05259644],\n",
      "       [-0.06395269],\n",
      "       [-0.02048986],\n",
      "       [-0.04708896],\n",
      "       [-0.0544408 ],\n",
      "       [-0.06497916],\n",
      "       [-0.02126176],\n",
      "       [-0.05079832],\n",
      "       [-0.05475704],\n",
      "       [-0.06400242],\n",
      "       [-0.02236493],\n",
      "       [-0.05158473],\n",
      "       [-0.05481111],\n",
      "       [-0.06866059],\n",
      "       [-0.01883668],\n",
      "       [-0.05258529],\n",
      "       [-0.05492185],\n",
      "       [-0.03967453],\n",
      "       [-0.02890478],\n",
      "       [-0.05423963],\n",
      "       [-0.05507626],\n",
      "       [-0.00091594],\n",
      "       [-0.03386811],\n",
      "       [-0.05492389],\n",
      "       [-0.05515006]]), 'b': 0.07288976115716832, 'learning_rate': 0.001, 'num_iterations': 10000}\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model_work = model(train_set_x_orig_work, train_set_y_work, test_set_x_orig_work, num_iterations=10000, learning_rate=0.001, print_cost=True)\n",
    "print(logistic_regression_model_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\source_data\\synthetic_contacts_2021.csv\")\n",
    "data=data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr=logistic_regression_model_all[\"Y_prediction_train\"]\n",
    "Yt=logistic_regression_model_all[\"Y_prediction_test\"]\n",
    "m=np.max(train_set_y_m_all)\n",
    "Yr=Yr*m\n",
    "Yt=Yt*m\n",
    "\n",
    "\n",
    "dfr = pd.DataFrame(Yr) \n",
    "dfr.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_train_all.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120)])\n",
    "\n",
    "dft= pd.DataFrame(Yt)\n",
    "dft.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_test_all.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120,150)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr=logistic_regression_model_home[\"Y_prediction_train\"]\n",
    "Yt=logistic_regression_model_home[\"Y_prediction_test\"]\n",
    "m=np.max(train_set_y_m_home)\n",
    "Yr=Yr*m\n",
    "Yt=Yt*m\n",
    "\n",
    "\n",
    "dfr = pd.DataFrame(Yr) \n",
    "dfr.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_train_home.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120)])\n",
    "\n",
    "dft= pd.DataFrame(Yt)\n",
    "dft.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_test_home.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120,150)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr=logistic_regression_model_others[\"Y_prediction_train\"]\n",
    "Yt=logistic_regression_model_others[\"Y_prediction_test\"]\n",
    "m=np.max(train_set_y_m_others)\n",
    "Yr=Yr*m\n",
    "Yt=Yt*m\n",
    "\n",
    "\n",
    "dfr = pd.DataFrame(Yr) \n",
    "dfr.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_train_others.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120)])\n",
    "\n",
    "dft= pd.DataFrame(Yt)\n",
    "dft.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_test_others.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120,150)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr=logistic_regression_model_school[\"Y_prediction_train\"]\n",
    "Yt=logistic_regression_model_school[\"Y_prediction_test\"]\n",
    "m=np.max(train_set_y_m_school)\n",
    "Yr=Yr*m\n",
    "Yt=Yt*m\n",
    "\n",
    "\n",
    "dfr = pd.DataFrame(Yr) \n",
    "dfr.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_train_school.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120)])\n",
    "\n",
    "dft= pd.DataFrame(Yt)\n",
    "dft.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_test_school.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120,150)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr=logistic_regression_model_work[\"Y_prediction_train\"]\n",
    "Yt=logistic_regression_model_work[\"Y_prediction_test\"]\n",
    "m=np.max(train_set_y_m_work)\n",
    "Yr=Yr*m\n",
    "Yt=Yt*m\n",
    "\n",
    "\n",
    "dfr = pd.DataFrame(Yr) \n",
    "dfr.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_train_work.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120)])\n",
    "\n",
    "dft= pd.DataFrame(Yt)\n",
    "dft.to_excel(r\"E:\\Hraf\\S8\\Projet à Enjeux\\Projet Enjeux\\normes predected\\norme_predeted_test_work.xlsx\", sheet_name='dataset',header=[data[((256-(16*(1+10)))+10)+256*(5*i)][0] for i in range(120,150)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
